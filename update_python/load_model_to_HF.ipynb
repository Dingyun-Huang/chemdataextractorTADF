{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from allennlp.models.archival import load_archive\n",
    "from chemdataextractor.data import find_data\n",
    "import json, os, appdirs\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "overrides = {\"model.text_field_embedder.token_embedders.bert.pretrained_model\": find_data(\"models/scibert_cased_weights-1.0.tar.gz\")}\n",
    "cde_bert_archive = load_archive(find_data('models/bert_finetuned_crf_model-1.0a'), overrides=json.dumps(overrides))\n",
    "cde_bertcrf_model = cde_bert_archive.model\n",
    "cde_bertcrf_model_state_dict = cde_bertcrf_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, PreTrainedModel, AutoConfig, PretrainedConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from chemdataextractor.nlp.crf import ConditionalRandomField, allowed_transitions\n",
    "from chemdataextractor.nlp.allennlp_modules import TimeDistributed\n",
    "from chemdataextractor.errors import ConfigurationError\n",
    "from typing import Dict, Optional, List, Tuple\n",
    "from overrides import overrides\n",
    "import numpy as np\n",
    "from chemdataextractor.nlp.util import (combine_initial_dims, get_device_of,\n",
    "                                        get_range_vector,\n",
    "                                        uncombine_initial_dims)\n",
    "\n",
    "class BertCrfConfig(PretrainedConfig):\n",
    "    model_type = 'bert'\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tags: int = 3,\n",
    "        dropout=0.1,\n",
    "        label_namespace: str = \"labels\",\n",
    "        label_encoding: Optional[str] = None,\n",
    "        index_and_label: List[Tuple[int, str]] = None,\n",
    "        constrain_crf_decoding: bool = True,\n",
    "        include_start_end_transitions: bool = True,\n",
    "        model_name_or_path: str = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.num_tags = num_tags\n",
    "        self.dropout = dropout\n",
    "        self.label_namespace = label_namespace\n",
    "        self.label_encoding = label_encoding\n",
    "        self.index_and_label = index_and_label\n",
    "        self.constrain_crf_decoding = constrain_crf_decoding\n",
    "        self.include_start_end_transitions = include_start_end_transitions\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "class BertCrfModel(PreTrainedModel):\n",
    "    config_class = BertCrfConfig  # Required for saving/loading\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__(config)\n",
    "        self.bert_model = AutoModel.from_config(\n",
    "            AutoConfig.from_pretrained(config.model_name_or_path))\n",
    "        self.num_tags = config.num_tags\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.tag_projection_layer = TimeDistributed(\n",
    "            nn.Linear(self.bert_model.config.hidden_size, self.num_tags, bias=True)\n",
    "        )\n",
    "\n",
    "        self.label_encoding = config.label_encoding\n",
    "        self.index_and_label = config.index_and_label\n",
    "        self.index_to_label = self._index_to_label()\n",
    "        self.label_to_index = self._label_to_index()\n",
    "\n",
    "        if config.constrain_crf_decoding:\n",
    "            if not config.label_encoding:\n",
    "                raise ConfigurationError(\"constrain_crf_decoding is True, but \"\n",
    "                                         \"no label_encoding was specified.\")\n",
    "            labels = self.index_to_label\n",
    "            constraints = allowed_transitions(config.label_encoding, labels)\n",
    "        else:\n",
    "            constraints = None\n",
    "\n",
    "        self.include_start_end_transitions = config.include_start_end_transitions\n",
    "        self.crf = ConditionalRandomField(\n",
    "            self.num_tags, constraints,\n",
    "            include_start_end_transitions=config.include_start_end_transitions\n",
    "        )\n",
    "\n",
    "\n",
    "    def _index_to_label(self):\n",
    "        return {index: label for index, label in self.index_and_label}\n",
    "\n",
    "    def _label_to_index(self):\n",
    "        return {label: index for index, label in self.index_and_label}\n",
    "\n",
    "    def forward(self, input_ids, offsets, crf_mask, token_type_ids=None):\n",
    "        # BERT embeddings\n",
    "        # print(input_ids.size())\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        input_mask = (input_ids != 0).long()\n",
    "\n",
    "        # input_ids may have extra dimensions, so we reshape down to 2-d\n",
    "        # before calling the BERT model and then reshape back at the end.\n",
    "        outputs = self.bert_model(input_ids=combine_initial_dims(input_ids),\n",
    "                                  token_type_ids=combine_initial_dims(\n",
    "                                      token_type_ids),\n",
    "                                  attention_mask=combine_initial_dims(input_mask))\n",
    "        # all_encoder_layers = torch.stack(outputs.last_hidden_state)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        last_hidden_state = self.dropout(last_hidden_state)\n",
    "        # At this point, mix is (batch_size * d1 * ... * dn, sequence_length, embedding_dim)\n",
    "        # offsets is (batch_size, d1, ..., dn, orig_sequence_length)\n",
    "        offsets2d = combine_initial_dims(offsets)\n",
    "        # now offsets is (batch_size * d1 * ... * dn, orig_sequence_length)\n",
    "        range_vector = get_range_vector(offsets2d.size(0),\n",
    "                                        device=get_device_of(last_hidden_state)).unsqueeze(1)\n",
    "        # selected embeddings is also (batch_size * d1 * ... * dn, orig_sequence_length)\n",
    "        selected_embeddings = last_hidden_state[range_vector, offsets2d]\n",
    "\n",
    "        output_embeddings = uncombine_initial_dims(\n",
    "            selected_embeddings, offsets.size())\n",
    "        print(\"HF output_embeddings\", output_embeddings)\n",
    "\n",
    "        # TODO: Sperate the function into two parts: one for the BERT embeddings and the other for the CRF\n",
    "        # print(sequence_output.size())\n",
    "        # Project onto tag space\n",
    "        logits = self.tag_projection_layer(output_embeddings)\n",
    "        best_paths = self.crf.viterbi_tags(logits, crf_mask)\n",
    "\n",
    "        predicted_tags = [x for x, y in best_paths]\n",
    "\n",
    "        output = {\"logits\": logits, \"mask\": crf_mask, \"tags\": predicted_tags}\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def forward_on_instances(self, instances: Dict[str, torch.Tensor]) -> List[Dict[str, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Takes a list of  :class:`~allennlp.data.instance.Instance`s, converts that text into\n",
    "        arrays using this model's :class:`Vocabulary`, passes those arrays through\n",
    "        :func:`self.forward()` and :func:`self.decode()` (which by default does nothing)\n",
    "        and returns the result.  Before returning the result, we convert any\n",
    "        ``torch.Tensors`` into numpy arrays and separate the\n",
    "        batched output into a list of individual dicts per instance. Note that typically\n",
    "        this will be faster on a GPU (and conditionally, on a CPU) than repeated calls to\n",
    "        :func:`forward_on_instance`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        instances : Dict[str, torch.Tensor], required\n",
    "            The instances to run the model on.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A list of the models output for each instance.\n",
    "        \"\"\"\n",
    "        batch_size = instances['input_ids'].size(0)\n",
    "        with torch.no_grad():\n",
    "            instances = {k: v.to(self.device) for k, v in instances.items()}\n",
    "            outputs = self.decode(self(**instances))\n",
    "\n",
    "            instance_separated_output: List[Dict[str, np.ndarray]] = [{} for _ in range(batch_size)]\n",
    "            for name, output in list(outputs.items()):\n",
    "                if isinstance(output, torch.Tensor):\n",
    "                    # NOTE(markn): This is a hack because 0-dim pytorch tensors are not iterable.\n",
    "                    # This occurs with batch size 1, because we still want to include the loss in that case.\n",
    "                    # if output.dim() == 0:\n",
    "                    #     output = output.unsqueeze(0)\n",
    "\n",
    "                    output = output.detach().cpu().numpy()\n",
    "                for instance_output, batch_element in zip(instance_separated_output, output):\n",
    "                    instance_output[name] = batch_element\n",
    "            return instance_separated_output\n",
    "\n",
    "    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Converts the tag ids to the actual tags.\n",
    "        ``output_dict[\"tags\"]`` is a list of lists of tag_ids,\n",
    "        so we use an ugly nested list comprehension.\n",
    "        \"\"\"\n",
    "        output_dict[\"tags\"] = [\n",
    "            [self.index_to_label[tag]\n",
    "             for tag in instance_tags]\n",
    "            for instance_tags in output_dict[\"tags\"]\n",
    "        ]\n",
    "        return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertcrf_config = BertCrfConfig(\n",
    "    num_tags=3,\n",
    "    label_namespace=\"labels\",\n",
    "    label_encoding=\"BIO\",\n",
    "    index_and_label=[(0, \"O\"), (1, \"I-CEM\"), (2, \"B-CEM\")],\n",
    "    constrain_crf_decoding=True,\n",
    "    include_start_end_transitions=False,\n",
    "    dropout=0.1,\n",
    "    model_name_or_path=\"allenai/scibert_scivocab_cased\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dh582/.local/share/ChemDataExtractor/models/hf_bert_crf_tagger\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(appdirs.user_data_dir('ChemDataExtractor'), 'models/hf_bert_crf_tagger')\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertcrf_config.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertcrf_tagger = BertCrfModel(bertcrf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {}\n",
    "for k, v in cde_bertcrf_model_state_dict.items():\n",
    "    if k.startswith(\"text_field_embedder.token_embedder_bert\"):\n",
    "        state_dict[k[40:]] = v\n",
    "    else:\n",
    "        state_dict[k] = v\n",
    "# print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertcrf_tagger.load_state_dict(state_dict=state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertcrf_tagger.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/dh582/.local/share/ChemDataExtractor/models/hf_bert_crf_tagger/tokenizer_config.json',\n",
       " '/home/dh582/.local/share/ChemDataExtractor/models/hf_bert_crf_tagger/special_tokens_map.json',\n",
       " '/home/dh582/.local/share/ChemDataExtractor/models/hf_bert_crf_tagger/vocab.txt',\n",
       " '/home/dh582/.local/share/ChemDataExtractor/models/hf_bert_crf_tagger/added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "hf_tokenizer = BertTokenizer(vocab_file=find_data('models/scibert_cased_vocab-1.0.txt'), do_lower_case=False)\n",
    "hf_tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import PretrainedBertIndexer\n",
    "allen_indexer = PretrainedBertIndexer(do_lowercase=False, use_starting_offsets=True, truncate_long_sequences=False, pretrained_model=find_data(\"models/scibert_cased_vocab-1.0.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chemdataextractor.doc import Sentence\n",
    "test_s = Sentence('2-(4-Chloro-2-fluoro-3-difluoromethylphenyl)-[1,3,2]-dioxaborinane 1H NMR (CDCl3):')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      " tensor([[  101,   957,   152, 30171, 30118,  1359,   578,   143,   957,   152,\n",
      "         30171, 30118,  1359,   578, 19732, 30110,   578,   957,   152, 30171,\n",
      "         30118,  1359,   578,  8225, 30110,   578,   957,   152, 30171, 30118,\n",
      "          1359,   578, 14972,  8086, 21532, 13981,   551,   578,   268,   957,\n",
      "           152, 30171, 30118,  1359,  1914,   578,   432,   783,  2321,  1923,\n",
      "           647,   155, 30155,  6052,   143, 15918, 30141,   551,   864,   102]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2', 'B-CM'),\n",
       " ('-', 'I-CM'),\n",
       " ('(', 'I-CM'),\n",
       " ('4', 'I-CM'),\n",
       " ('-', 'I-CM'),\n",
       " ('Chloro', 'I-CM'),\n",
       " ('-', 'I-CM'),\n",
       " ('2', 'I-CM'),\n",
       " ('-', 'I-CM'),\n",
       " ('fluoro', 'I-CM'),\n",
       " ('-', 'I-CM'),\n",
       " ('3', 'I-CM'),\n",
       " ('-', 'I-CM'),\n",
       " ('difluoromethylphenyl', 'I-CM'),\n",
       " (')', 'I-CM'),\n",
       " ('-', 'I-CM'),\n",
       " ('[', 'I-CM'),\n",
       " ('1,3,2', 'I-CM'),\n",
       " (']', 'I-CM'),\n",
       " ('-', 'I-CM'),\n",
       " ('dioxaborinane', 'I-CM'),\n",
       " ('1H', 'O'),\n",
       " ('NMR', 'O'),\n",
       " ('(', None),\n",
       " ('CDCl3', None),\n",
       " (')', None),\n",
       " (':', 'O')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_s.ner_tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cde-update",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
